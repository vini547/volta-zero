{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db91935",
   "metadata": {},
   "source": [
    "## 5 Diferenças entre AdaBoost e GBM (Gradient Boosting Machine)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Forma de corrigir erros\n",
    "- **AdaBoost:** Dá mais peso para exemplos mal classificados, focando nos erros do modelo anterior.\n",
    "- **GBM:** Minimiza uma função de perda via gradiente descendente, ajustando os resíduos dos modelos anteriores.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Tipo de modelos base\n",
    "- **AdaBoost:** Geralmente usa árvores rasas (stumps) com profundidade 1.\n",
    "- **GBM:** Usa árvores um pouco mais profundas, capturando padrões mais complexos.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Flexibilidade na função de perda\n",
    "- **AdaBoost:** Projetado para classificação, com perda exponencial.\n",
    "- **GBM:** Permite várias funções de perda (regressão, classificação, ranking), sendo mais flexível.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Peso dos exemplos e amostragem\n",
    "- **AdaBoost:** Ajusta explicitamente os pesos dos exemplos a cada iteração.\n",
    "- **GBM:** Trabalha ajustando os resíduos sem alterar diretamente os pesos dos exemplos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Velocidade e complexidade\n",
    "- **AdaBoost:** Mais simples e rápido, devido aos modelos base mais simples.\n",
    "- **GBM:** Mais lento e poderoso, pois realiza otimização numérica e usa árvores mais complexas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110f4a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b53f8d",
   "metadata": {},
   "source": [
    "## 5 Hiperparâmetros Importantes no GBM e Por Que São Essenciais\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `n_estimators`  \n",
    "- **O que faz:** Número de árvores a serem treinadas sequencialmente.  \n",
    "- **Por que é importante:** Controla a complexidade do modelo. Poucos estimadores podem causar underfitting; muitos podem causar overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `learning_rate` (taxa de aprendizado)  \n",
    "- **O que faz:** Escala a contribuição de cada árvore adicionada.  \n",
    "- **Por que é importante:** Regulariza o aprendizado. Taxas menores exigem mais árvores, mas ajudam a evitar overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `max_depth`  \n",
    "- **O que faz:** Profundidade máxima de cada árvore.  \n",
    "- **Por que é importante:** Controla o nível de complexidade e interação capturada por cada árvore. Profundidades muito altas podem causar overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `min_samples_split` / `min_child_weight`  \n",
    "- **O que faz:** Número mínimo de amostras necessárias para dividir um nó.  \n",
    "- **Por que é importante:** Impede que a árvore cresça demais com divisões baseadas em poucos dados, ajudando a evitar overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `subsample`  \n",
    "- **O que faz:** Fração dos dados usados para treinar cada árvore (amostragem sem reposição).  \n",
    "- **Por que é importante:** Introduz aleatoriedade, reduzindo correlação entre árvores e ajudando na generalização do modelo.\n",
    "\n",
    "---\n",
    "\n",
    " Ajustar bem esses hiperparâmetros é fundamental para equilibrar a **capacidade de aprendizado** do modelo com sua **capacidade de generalização**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e86c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Acurácia no conjunto de teste: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Carregar dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2. Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Definir o modelo GBM\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 4. Definir grade de hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 5. Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# 6. Ajustar o GridSearch no conjunto de treino\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Mostrar melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 8. Avaliar o melhor modelo no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia no conjunto de teste: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2c41b",
   "metadata": {},
   "source": [
    "## Diferenças entre GBM clássico e Stochastic Gradient Boosting (Friedman)\n",
    "\n",
    "### GBM clássico\n",
    "- Treina cada árvore sequencialmente usando **todo o conjunto de dados** para calcular os resíduos.\n",
    "- Cada árvore tenta corrigir os erros da soma das anteriores, minimizando a função de perda.\n",
    "- Pode ser eficiente, mas tende a gerar árvores correlacionadas (alta variância).\n",
    "\n",
    "### Stochastic Gradient Boosting\n",
    "- Introduz **aleatoriedade (stocasticidade)** no processo de treinamento.\n",
    "- Em vez de usar todo o conjunto, a cada iteração treina cada árvore em uma **amostra aleatória (subamostra)** dos dados, sem reposição.\n",
    "- Essa amostragem reduz a correlação entre árvores e ajuda a melhorar a generalização.\n",
    "- Geralmente melhora a robustez do modelo e reduz overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## O que são processos estocásticos?\n",
    "\n",
    "- São processos que envolvem **elementos de aleatoriedade** ou incerteza.\n",
    "- Em aprendizado de máquina, introduzir estocasticidade significa **usar amostras aleatórias**, ruído ou outras variações probabilísticas para tornar o modelo menos determinístico.\n",
    "- No caso do Stochastic GBM, a amostragem aleatória dos dados a cada iteração é um exemplo clássico de um processo estocástico.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que a estocasticidade ajuda no GBM?\n",
    "\n",
    "- Reduz a correlação entre as árvores do ensemble, aumentando a diversidade.\n",
    "- Evita que o modelo ajuste excessivamente os dados de treino (overfitting).\n",
    "- Pode melhorar a velocidade e a estabilidade do aprendizado.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumo rápido:\n",
    "\n",
    "| Aspecto                 | GBM Clássico           | Stochastic GBM                        |\n",
    "|------------------------|-----------------------|-------------------------------------|\n",
    "| Uso dos dados          | Usa todo o dataset     | Usa subamostra aleatória (ex: 50%)  |\n",
    "| Correlação entre árvores | Alta                  | Menor, mais diversidade             |\n",
    "| Overfitting            | Maior risco            | Menor risco                         |\n",
    "| Estocasticidade        | Não                    | Sim                                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceddf42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia GBM clássico: 1.0000\n",
      "Acurácia Stochastic GBM: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting clássico (subsample=1.0)\n",
    "gbm_classic = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                         max_depth=3, subsample=1.0, random_state=42)\n",
    "gbm_classic.fit(X_train, y_train)\n",
    "pred_classic = gbm_classic.predict(X_test)\n",
    "acc_classic = accuracy_score(y_test, pred_classic)\n",
    "print(f\"Acurácia GBM clássico: {acc_classic:.4f}\")\n",
    "\n",
    "# Stochastic Gradient Boosting (subsample < 1.0, ex: 0.5)\n",
    "gbm_stochastic = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                            max_depth=3, subsample=0.5, random_state=42)\n",
    "gbm_stochastic.fit(X_train, y_train)\n",
    "pred_stochastic = gbm_stochastic.predict(X_test)\n",
    "acc_stochastic = accuracy_score(y_test, pred_stochastic)\n",
    "print(f\"Acurácia Stochastic GBM: {acc_stochastic:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
