{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af15fc51",
   "metadata": {},
   "source": [
    "##  Diferen√ßas entre Random Forest e AdaBoost\n",
    "\n",
    "Aqui est√£o 5 diferen√ßas entre os algoritmos **Random Forest** e **AdaBoost**, explicadas de forma leve e natural:\n",
    "\n",
    "---\n",
    "\n",
    "### 1.  Jeito que combinam os modelos\n",
    "- **Random Forest**: Treina v√°rias √°rvores de forma independente e combina os resultados por **vota√ß√£o (classifica√ß√£o)** ou **m√©dia (regress√£o)**.\n",
    "- **AdaBoost**: Treina modelos **um ap√≥s o outro**, e cada novo modelo tenta corrigir os erros do anterior. No final, cada modelo tem um **peso** diferente na decis√£o final.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.  Como lidam com os dados de treino\n",
    "- **Random Forest**: Usa partes aleat√≥rias dos dados com **reposi√ß√£o** para treinar cada √°rvore. Cada uma v√™ um \"mundo diferente\".\n",
    "- **AdaBoost**: Come√ßa tratando todos os dados igual, mas com o tempo d√° **mais peso para os exemplos dif√≠ceis**, focando nos que foram mal classificados.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.  Independ√™ncia entre os modelos\n",
    "- **Random Forest**: As √°rvores s√£o **independentes** entre si. Uma n√£o sabe o que a outra fez.\n",
    "- **AdaBoost**: Os modelos s√£o **dependentes**. Um modelo aprende com os erros do anterior.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.  Sensibilidade a ru√≠dos e outliers\n",
    "- **Random Forest**: Mais **robusto**. Como n√£o tenta acertar tudo a qualquer custo, lida melhor com ru√≠do e valores extremos.\n",
    "- **AdaBoost**: Mais **sens√≠vel**. Insiste nos exemplos dif√≠ceis e pode acabar exagerando nos ru√≠dos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.  Tend√™ncia ao overfitting\n",
    "- **Random Forest**: Em geral, √© **resistente ao overfitting**, especialmente com n√∫mero alto de √°rvores e regulariza√ß√£o.\n",
    "- **AdaBoost**: Pode **sofrer mais com overfitting**, principalmente se os dados tiverem muito ru√≠do ou forem mal balanceados.\n",
    "\n",
    "---\n",
    "\n",
    " Ambos s√£o poderosos, mas t√™m personalidades bem diferentes. O ideal √© testar os dois e ver quem se sai melhor com os seus dados!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4cc241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3e84d",
   "metadata": {},
   "source": [
    "## üîß Principais Par√¢metros do AdaBoostClassifier\n",
    "\n",
    "Abaixo est√£o os **5 par√¢metros mais importantes** do `AdaBoostClassifier` do scikit-learn, com uma breve explica√ß√£o de **o que fazem** e **por que s√£o importantes**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1.  `n_estimators`\n",
    "- **O que √©:** N√∫mero de modelos fracos (weak learners) treinados em sequ√™ncia.\n",
    "- **Por que √© importante:**  \n",
    "  - Controla quantas rodadas de boosting o modelo realiza.\n",
    "  - Poucos estimadores ‚Üí underfitting.  \n",
    "  - Muitos estimadores ‚Üí risco de overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.  `learning_rate`\n",
    "- **O que √©:** Fator de escala para o peso de cada modelo fraco na previs√£o final.\n",
    "- **Por que √© importante:**  \n",
    "  - Atua como regularizador: quanto menor, mais suave o aprendizado.\n",
    "  - Um `learning_rate` menor normalmente exige mais estimadores (`n_estimators`).\n",
    "  - Pode melhorar a generaliza√ß√£o se bem ajustado.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.  `estimator` (antigo `base_estimator`)\n",
    "- **O que √©:** O tipo de modelo fraco utilizado (por padr√£o, `DecisionTreeClassifier(max_depth=1)`).\n",
    "- **Por que √© importante:**  \n",
    "  - Define a complexidade de cada modelo da sequ√™ncia.\n",
    "  - Pode usar modelos mais fortes, como √°rvores com maior profundidade.\n",
    "  - Impacta a velocidade e a capacidade de aprendizado do AdaBoost.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.  `algorithm`\n",
    "- **O que √©:** Estrat√©gia de boosting: `\"SAMME\"` ou `\"SAMME.R\"` (default: `\"SAMME.R\"`).\n",
    "- **Por que √© importante:**  \n",
    "  - `\"SAMME.R\"` usa probabilidades (valores reais) ‚Üí geralmente mais r√°pido e mais preciso.\n",
    "  - `\"SAMME\"` usa apenas r√≥tulos de classe ‚Üí pode ser mais lento ou menos est√°vel.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.  `random_state`\n",
    "- **O que √©:** Semente de aleatoriedade para garantir resultados reprodut√≠veis.\n",
    "- **Por que √© importante:**  \n",
    "  - N√£o afeta a performance diretamente, mas essencial para **reproduzir experimentos**.\n",
    "  - √ötil ao comparar modelos ou fazer tuning.\n",
    "\n",
    "---\n",
    "\n",
    "###  Tabela Resumo\n",
    "\n",
    "| Par√¢metro        | Controla                         | Import√¢ncia Principal                             |\n",
    "|------------------|----------------------------------|---------------------------------------------------|\n",
    "| `n_estimators`   | N√∫mero de modelos fracos         | Balanceia underfitting e overfitting              |\n",
    "| `learning_rate`  | Peso de cada estimador           | Regula a influ√™ncia e ajuda a evitar overfitting  |\n",
    "| `estimator`      | Tipo de modelo base              | Determina o \"poder\" de cada modelo individual     |\n",
    "| `algorithm`      | Estrat√©gia de boosting           | Afeta velocidade e precis√£o                       |\n",
    "| `random_state`   | Aleatoriedade                    | Garante reprodutibilidade                         |\n",
    "\n",
    "---\n",
    "\n",
    " Esses par√¢metros formam o cora√ß√£o do AdaBoost. Saber ajust√°-los √© essencial para tirar o m√°ximo proveito do algoritmo!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c6cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperpar√¢metros encontrados:\n",
      "{'estimator__max_depth': 1, 'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Acur√°cia no conjunto de teste: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Carregar dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2. Dividir dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Modelo base\n",
    "base_tree = DecisionTreeClassifier()\n",
    "\n",
    "# 4. AdaBoost\n",
    "ada = AdaBoostClassifier(estimator=base_tree, random_state=42)\n",
    "\n",
    "# 5. Grade de hiperpar√¢metros (sem 'algorithm')\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'estimator__max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# 6. GridSearchCV\n",
    "grid_search = GridSearchCV(ada, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Resultados\n",
    "print(\"Melhores hiperpar√¢metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 8. Avalia√ß√£o no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acur√°cia no conjunto de teste: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
